import gradio as gr
import pandas as pd
import plotly.graph_objects as go
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Function to preprocess the data
def preprocess_data(df):
    """Handle missing values and perform necessary preprocessing."""
    for column in df.select_dtypes(include=['float64', 'int64']).columns:
        df[column] = pd.to_numeric(df[column], errors='coerce')
        df[column].fillna(df[column].mean(), inplace=True)

    if 'SeniorCitizen' in df.columns:
        df['SeniorCitizen'] = df['SeniorCitizen'].map({0: "No", 1: "Yes"})
    
    return df

# Function to encode categorical columns
def encode_categorical_columns(df):
    """Encode categorical columns into numeric format."""
    for column in df.select_dtypes(include=['object']).columns:
        df[column] = LabelEncoder().fit_transform(df[column])
    return df

# Function to generate churn rate pie chart
def plot_churn_rate(df):
    """Generate a pie chart of churn rate distribution."""
    churn_column = identify_churn_column(df)
    
    if churn_column is None:
        return "No churn-related column detected in the dataset!"
    
    churn_values = df[churn_column].value_counts()
    
    # If churn values are numeric (0/1), convert them to Yes/No labels
    if set(churn_values.index) == {0, 1}:
        churn_labels = ['Churn: No', 'Churn: Yes']
    else:
        churn_labels = churn_values.index.tolist()

    fig = go.Figure(data=[go.Pie(labels=churn_labels, values=churn_values, hole=.3)])
    fig.update_layout(title_text="Churn Rate Distribution", showlegend=True)
    return fig

# Function to identify the churn column in the dataset
def identify_churn_column(df):
    """Automatically identify the churn column from the dataset."""
    potential_columns = ['Churn', 'Exited', 'Target', 'Attrition', 'Churned', 'IsChurn', 'customer_churn']

    for column in df.columns:
        if column in potential_columns:
            return column
        unique_values = df[column].dropna().unique()
        if set(unique_values) == {0, 1} or set(unique_values) == {'Yes', 'No'}:
            return column
    
    return None

# Function for data analysis
def data_analysis(df):
    """Perform basic data analysis and visualization."""
    corr = df.corr()
    fig, ax = plt.subplots(figsize=(10, 8))
    sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=".2f", ax=ax)
    heatmap = plt.gcf()

    distribution_plots = []
    for column in df.select_dtypes(include=['float64', 'int64']).columns:
        fig, ax = plt.subplots(figsize=(8, 5))
        sns.histplot(df[column], kde=True, ax=ax)
        ax.set_title(f'Distribution of {column}')
        distribution_plots.append(plt.gcf())
    
    return heatmap, distribution_plots

# Function to plot confusion matrix
def plot_confusion_matrix(cm):
    fig_cm, ax = plt.subplots(figsize=(6, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, cbar=False, annot_kws={"size": 16})
    ax.set_title("Confusion Matrix", fontsize=16)
    ax.set_xlabel('Predicted', fontsize=14)
    ax.set_ylabel('Actual', fontsize=14)
    return fig_cm

# Function to plot feature importance
def plot_feature_importance(features, importances):
    """Plot a bar chart for feature importance."""
    feature_importance_df = pd.DataFrame({
        'Feature': features,
        'Importance': importances
    })
    
    feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)
    
    fig, ax = plt.subplots(figsize=(10, 6))
    sns.barplot(x='Importance', y='Feature', data=feature_importance_df, ax=ax, palette='viridis')
    ax.set_title("Feature Importance")
    ax.set_xlabel("Importance")
    ax.set_ylabel("Feature")
    
    return fig

# Gradio Interface Functions
def upload_and_preview(file):
    if file is not None:
        df = pd.read_csv(file.name)
        return df.head()

def train_model(file, selected_features):
    if file is None:
        return "Please upload a dataset first."
    
    df = pd.read_csv(file.name)
    target_column = identify_churn_column(df)
    
    if target_column is None:
        return "No churn-related column detected in the dataset."

    df = preprocess_data(df)
    df = encode_categorical_columns(df)
    features = df[selected_features]
    target = df[target_column]

    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.30, random_state=42, stratify=target)
    
    # Standardize numerical columns
    scaler = StandardScaler()
    X_train[selected_features] = scaler.fit_transform(X_train[selected_features])
    X_test[selected_features] = scaler.transform(X_test[selected_features])
    
    # Random Forest Classifier
    rf_model = RandomForestClassifier(n_estimators=500, random_state=42)
    rf_model.fit(X_train, y_train)

    predictions = rf_model.predict(X_test)

    accuracy = accuracy_score(y_test, predictions)

    confusion_mat = confusion_matrix(y_test, predictions)
    feature_importances = rf_model.feature_importances_

    cm_plot = plot_confusion_matrix(confusion_mat)
    feature_importance_plot = plot_feature_importance(selected_features, feature_importances)

    return {
        "Accuracy": f"{accuracy*100:.2f}%",
        "Confusion Matrix": cm_plot,
        "Feature Importance": feature_importance_plot
    }

def analyze_data(file):
    if file is None:
        return "Please upload a dataset first."
    
    df = pd.read_csv(file.name)
    heatmap, distributions = data_analysis(df)
    return {
        "Correlation Heatmap": heatmap,
        "Numerical Feature Distributions": distributions
    }

# Gradio Interface
def create_interface():
    # Upload Data Interface
    upload_input = gr.File(label="Upload Your Dataset (CSV)", type="filepath")
    feature_input = gr.CheckboxGroup(label="Select Features for Prediction", choices=["Feature1", "Feature2", "Feature3", "Feature4"], type="value")

    # Model Training
    train_button = gr.Button("Train Model")
    train_output = gr.Textbox(label="Model Results", lines=10)

    train_button.click(train_model, inputs=[upload_input, feature_input], outputs=train_output)

    # Data Analysis
    analyze_button = gr.Button("Analyze Data")
    analyze_output = gr.Plot()

    analyze_button.click(analyze_data, inputs=[upload_input], outputs=analyze_output)

    # Layout
    interface = gr.Interface(
        fn=upload_and_preview,
        inputs=[upload_input],
        outputs=[gr.Dataframe()],
        live=True
    )

    return interface

# Run the Gradio app
if __name__ == "__main__":
    create_interface().launch()
